<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
               "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>The Norvig - Chomsky debate</title>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
<meta name="title" content="The Norvig - Chomsky debate"/>
<meta name="generator" content="Org-mode"/>
<meta name="generated" content="2012-05-30 09:00:22 EDT"/>
<meta name="author" content="Joshua Eckroth"/>
<meta name="description" content=""/>
<meta name="keywords" content=""/>
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  html { font-family: Times, serif; font-size: 12pt; }
  .title  { text-align: center; }
  .todo   { color: red; }
  .done   { color: green; }
  .tag    { background-color: #add8e6; font-weight:normal }
  .target { }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  {margin-left:auto; margin-right:0px;  text-align:right;}
  .left   {margin-left:0px;  margin-right:auto; text-align:left;}
  .center {margin-left:auto; margin-right:auto; text-align:center;}
  p.verse { margin-left: 3% }
  pre {
	border: 1pt solid #AEBDCC;
	background-color: #F3F5F7;
	padding: 5pt;
	font-family: courier, monospace;
        font-size: 90%;
        overflow:auto;
  }
  table { border-collapse: collapse; }
  td, th { vertical-align: top;  }
  th.right  { text-align:center;  }
  th.left   { text-align:center;   }
  th.center { text-align:center; }
  td.right  { text-align:right;  }
  td.left   { text-align:left;   }
  td.center { text-align:center; }
  dt { font-weight: bold; }
  div.figure { padding: 0.5em; }
  div.figure p { text-align: center; }
  div.inlinetask {
    padding:10px;
    border:2px solid gray;
    margin:10px;
    background: #ffffcc;
  }
  textarea { overflow-x: auto; }
  .linenr { font-size:smaller }
  .code-highlighted {background-color:#ffff00;}
  .org-info-js_info-navigation { border-style:none; }
  #org-info-js_console-label { font-size:10px; font-weight:bold;
                               white-space:nowrap; }
  .org-info-js_search-highlight {background-color:#ffff00; color:#000000;
                                 font-weight:bold; }
  /*]]>*/-->
</style>
<link rel="stylesheet" href="css/worg.css" type="text/css" media="screen" />
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>

</head>
<body>

<div id="preamble">
<a href="index.html">Home</a> &nbsp; &nbsp;
          <!-- Plupper Button -->
          <div id="plupperButton" style="display: inline;"></div>
          <!-- End of Plupper Button Code -->
          <!-- Plupper Tracking Code -->
          <script src="https://www.google.com/jsapi"></script>
          <script type="text/javascript"
                  src="https://static.plupper.com/js/plupper.js"></script>
          <script type="text/javascript">
            plupper.init("joshuaeckroth@plupper.com");
            plupper.enableCobrowsing();
          </script>
          <!-- End of Plupper Tracking Code -->

</div>

<div id="content">
<h1 class="title">The Norvig - Chomsky debate</h1>

<p>Recently, Peter Norvig took the time to write a response to a comment
Noam Chomsky made at a conference (not a comment made to Norvig, but
to an entire research community of which Norvig considers himself a
part).
</p>
<p>
Chomsky is (in)famous for hypothesizing and arguing that all humans
have a tacit but unlearned knowledge of linguistic structure, a
universal grammar. He believes the evidence for this hypothesis is
that children cannot possibly learn all that they do about their first
language just from what they hear. Rather, the structure of language
is so deep and sophisticated that children must already have the
mental structures needed, and do not learn these structures from
experience. An important structure, for example, is the ability to
understand recursive utterances, such as:
</p>
<blockquote>

<p>My homework assignment, which is worth 100 points in my CSE 630 class,
which is not required for my major but I wanted to take it anyway,
which has turned out to be quite interesting as it happens, is due
Thursday.
</p>
</blockquote>


<p>
Although that sentence is a bit contrived, we can understand it
(spoken or written). There are limits to how much recursive structure
we can keep in our short-term memory, but there is clearly (or not?) a
logic to it. How does a child learn this logic?
</p>
<p>
Norvig distinguishes language generation from language interpretation
(I do not know if this is a common or, alternatively, a pathological
distinction):
</p>
<blockquote>

<p>I understand how Chomsky arrives at the conclusion that probabilistic
models are unnecessary, from his study of the generation of
language. But the vast majority of people who study interpretation
tasks, such as speech recognition, quickly see that interpretation is
an inherently probabilistic problem: given a stream of noisy input to
my ears, what did the speaker most likely mean? [&hellip;] Many phenomena
in science are stochastic, and the simplest model of them is a
probabilistic model; I believe language is such a phenomenon and
therefore that probabilistic models are our best tool for representing
facts about language, for algorithmically processing language, and for
understanding how humans process language.
</p>
</blockquote>


<p>
First I would like to add that it is not true that only probabilistic
methods can handle noise. Noise can be removed or ignored with rules
as well.
</p>
<p>
He also states that language is a stochastic phenomenon. This strikes
me as fundamentally confused.
</p>

<div style="text-align: center">
<p><img src="./images/kumi-yamashita-building-blocks.jpg"  alt="./images/kumi-yamashita-building-blocks.jpg" />
</p>
<p>
&ldquo;Building Blocks,&rdquo; <a href="http://www.kumiyamashita.com/portfolio/building-blocks/">Kumi Yamashita</a>, 1997; H230, W400, D5cm; wood,
single light source, shadow
</p>
</div>



<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">&ldquo;i before e except after c&rdquo;</a></li>
<li><a href="#sec-2">Justice handshakes</a></li>
<li><a href="#sec-3">Success metrics</a></li>
<li><a href="#sec-4">Science: modeling or insight?</a></li>
<li><a href="#sec-5">Are statistical models ok?</a></li>
<li><a href="#sec-6">Statistical models for language acquisition</a>
<ul>
<li>
<ul>
<li><a href="#sec-6-1">The phenomenon</a></li>
<li><a href="#sec-6-2">A critique</a></li>
</ul></li>
</ul>
</li>
<li><a href="#sec-7">The edge of understanding</a></li>
</ul>
</div>
</div>

<div id="outline-container-1" class="outline-2">
<h2 id="sec-1">&ldquo;i before e except after c&rdquo;</h2>
<div class="outline-text-2" id="text-1">





<pre class="example">P(IE) = 0.0177         P(CIE) = 0.0014        P(*IE) = 0.163
P(EI) = 0.0046         P(CEI) = 0.0005        P(*EI) = 0.0041
</pre>


<p>
This model comes from statistics on a corpus of a trillion words of
English text. The notation P(IE) is the probability that a word
sampled from this corpus contains the consecutive letters &ldquo;IE.&rdquo; P(CIE)
is the probability that a word contains the consecutive letters &ldquo;CIE&rdquo;,
and P(*IE) is the probability of any letter other than C followed by
IE. The statistical data confirms that IE is in fact more common than
EI, and that the dominance of IE lessens when following a C, but
contrary to the rule, CIE is still more common than CEI. Examples of
&ldquo;CIE&rdquo; words include &ldquo;science,&rdquo; &ldquo;society,&rdquo; &ldquo;ancient&rdquo; and &ldquo;species.&rdquo; The
disadvantage of the &ldquo;I before E except after C&rdquo; model is that it is
not very accurate.
</p>
</div>

</div>

<div id="outline-container-2" class="outline-2">
<h2 id="sec-2">Justice handshakes</h2>
<div class="outline-text-2" id="text-2">


<p>
Reasons to prefer different representations of a solution. Lookup
table vs. a causal explanation (rules).
</p>
</div>

</div>

<div id="outline-container-3" class="outline-2">
<h2 id="sec-3">Success metrics</h2>
<div class="outline-text-2" id="text-3">


<p>
Statistical approaches are generally the most successful in various
linguistics problems.
</p>
</div>

</div>

<div id="outline-container-4" class="outline-2">
<h2 id="sec-4">Science: modeling or insight?</h2>
<div class="outline-text-2" id="text-4">


<blockquote>

<p>My conclusion is that 100% of these articles and awards are more about
&ldquo;accurately modeling the world&rdquo; than they are about &ldquo;providing
insight,&rdquo; although they all have some theoretical insight component as
well.
</p>
</blockquote>


</div>

</div>

<div id="outline-container-5" class="outline-2">
<h2 id="sec-5">Are statistical models ok?</h2>
<div class="outline-text-2" id="text-5">


<blockquote>

<p>Every probabilistic model is a superset of a deterministic model
(because the deterministic model could be seen as a probabilistic
model where the probabilities are restricted to be 0 or 1), so any
valid criticism of probabilistic models would have to be because they
are too expressive, not because they are not expressive enough.
</p>
</blockquote>


<p>
This is a bit disingenuous. Probabilisitic models rarely express
complex, deep relationships among data because such relationships are
hard to discover via a hands-off training process. On the other hand,
rule-based systems often have quite complex and deep rules (because,
apparently, sometimes such rules are necessary), which are made
possible by the fact that a human is creating them.
</p>
<blockquote>

<p>And yes, it seems clear that an adult speaker of English does know
billions of language facts (for example, that one says &ldquo;big game&rdquo;
rather than &ldquo;large game&rdquo; when talking about an important football
game). These facts must somehow be encoded in the brain.
</p>
</blockquote>


<p>
Fair enough. There does seem to be a statistical bias in language
generation, like the feeling of a word just rolling off the tongue,
before you have thought about it. That effect is probably what
produces &ldquo;big game&rdquo; rather than &ldquo;large game.&rdquo;
</p>
<blockquote>

<p>For example, the verb <i>quake</i> is listed as intransitive in
dictionaries, meaning that (1) below is grammatical, and (2) is not,
according to a categorical theory of grammar.
</p>
<ol>
<li>The earth quaked.
</li>
<li>? It quaked her bowels. 
</li>
</ol>


<p>
But (2) actually appears as a sentence of English. This poses a
dilemma for the categorical theory.
</p>
</blockquote>


<blockquote>

<p>Chomsky: &ldquo;You can also collect butterflies and make many
observations. If you like butterflies, that&rsquo;s fine; but such work must
not be confounded with research, which is concerned to discover
explanatory principles.&rdquo;
</p>
</blockquote>



<blockquote>

<p>From the beginning, Chomsky has focused on the generative side of
language. From this side, it is reasonable to tell a non-probabilistic
story: I know definitively the idea I want to express&mdash;I&rsquo;m starting
from a single semantic form&mdash;thus all I have to do is choose the
words to say it; why can&rsquo;t that be a deterministic, categorical
process? If Chomsky had focused on the other side, interpretation, as
Claude Shannon did, he may have changed his tune. In interpretation
(such as speech recognition) the listener receives a noisy, ambiguous
signal and needs to decide which of many possible intended messages is
most likely. Thus, it is obvious that this is inherently a
probabilistic problem[&hellip;]
</p>
</blockquote>


<p>
Why the distinction?
</p>
<blockquote>

<p>We also now know that language is like that as well: languages are
complex, random, contingent biological processes that are subject to
the whims of evolution and cultural change. What constitutes a
language is not an eternal ideal form, represented by the settings of
a small number of parameters, but rather is the contingent outcome of
complex processes. Since they are contingent, it seems they can only
be analyzed with probabilistic models. Since people have to
continually understand the uncertain. ambiguous, noisy speech of
others, it seems they must be using something like probabilistic
reasoning. Chomsky for some reason wants to avoid this, and therefore
he must declare the actual facts of language use out of bounds and
declare that true linguistics only exists in the mathematical realm,
where he can impose the formalism he wants. Then, to get language from
this abstract, eternal, mathematical realm into the heads of people,
he must fabricate a mystical facility that is exactly tuned to the
eternal realm. This may be very interesting from a mathematical point
of view, but it misses the point about what language is, and how it
works.
</p>
</blockquote>


<p>
But is it not equally true that a statistical conception of language
likewise requires a fantastically rich and sophisticated model in
order to generate and understand highly-structured, subtle,
carefully-crafted utterances that intelligent humans are known for?
Again, there is a missing link, no proof-by-demonstration.
</p>
</div>

</div>

<div id="outline-container-6" class="outline-2">
<h2 id="sec-6">Statistical models for language acquisition</h2>
<div class="outline-text-2" id="text-6">


<blockquote>

<p>On the evidence that what we will and won&rsquo;t say and what we will and
won&rsquo;t accept can be characterized by rules, it has been argued that,
in some sene, we &ldquo;know&rdquo; the rules of our language. The sense in which
we know them is not the same as the sense in which we know such
&ldquo;rules&rdquo; as &ldquo;<i>i</i> before <i>e</i> except after <i>c</i>,&rdquo; however, since we need
not necessarily be able to state the rules explicitly. We know them in
a way that allows us to use them to make judgments of grammaticality,
it is often said, or to speak and understand, but this knowledge is
not in a form or location that permits it to be encoded into a
communicable verbal statement. &mdash; &ldquo;On Learning the Past Tenses of
English Verbs,&rdquo; Rumelhart and McClelland (1986)
</p>
</blockquote>



</div>

<div id="outline-container-6-1" class="outline-4">
<h4 id="sec-6-1">The phenomenon</h4>
<div class="outline-text-4" id="text-6-1">


<blockquote>

<p>In Stage 1, children use only a small number of verbs in the past
tense. Such verbs tend to be very high-frequency words, and the
majority of these are irregular. At this stage, children tend to get
the past tenses of these words correct if they use the past tense at
all.
</p>
<p>
[&hellip;]
</p>
<p>
In Stage 2, evidence of implicit knowledge of a linguistic rule
emerges. At this stage, children use a much larger number of verbs in
the past tense. These verbs include a few more irregular items, but it
turns out that the majority of the words at this stage are examples of
the <i>regular</i> past tense in English. [&hellip;]
</p>
<ul>
<li>The child can now generate a past tense for an invented word.

</li>
<li>Children now <i>incorrectly</i> supply regular past-tense endings for
    words which they used correctly in Stage 1. These errors may
    involve either adding <i>ed</i> to the root as in <i>comed</i>, or adding
    <i>ed</i> to the irregular past tense form as in <i>camed</i>.
</li>
</ul>


<p>
[&hellip;]
</p>
<p>
In Stage 3, the regular and irregular forms coexist. That is, children
have regained the use of the correct irregular forms of the past
tense, while they continue to apply the regular form to new words they
learn. Regularization persists into adulthood [&hellip;]
</p>
</blockquote>


<p>
The authors further add that the stages are not well-delineated, and
that the acquisition process is gradual.
</p>
<p>
They develop a statistical neural network model that, when given
example verbs (like a child hearing them used), produces behavior
exactly like that described in the three stages (above).
</p>
</div>

</div>

<div id="outline-container-6-2" class="outline-4">
<h4 id="sec-6-2">A critique</h4>
<div class="outline-text-4" id="text-6-2">


<blockquote>

<p>Classical models [&hellip;] account for this [phenomenon] in an intuitively
obvious way. They posit an initial stage in which the child has
effectively memorized a small set of forms in a totally unsystematic
and unconnected way. This is stage one. At stage two, according to
this story, the child manages to extract a rule covering a large
number of cases. But the rule is now mistakenly deployed to generate
<i>all</i> past tenses. At the final stage this is put right. Now the child
uses lexical, memorized, item-indexed resources to handle irregular
cases and non-lexical, rule-based resources to handle regular
ones. &mdash; &ldquo;Critique of Rumelhart and McClelland,&rdquo; Andy Clark (1989)
</p>
</blockquote>


<p>
Problems wiht the statistical neural network model (according to
Clark, who is summarizing &ldquo;On language and connectionism,&rdquo; Pinker and
Prince (1988)):
</p>
<dl>
<dt>Overreliance on the environment</dt><dd>the stage 1 to stage 2
       transition of the model was achieved solely because only
       irregular verbs were presented to it first, followed by a
       &ldquo;massive influx&rdquo; of regular verbs; generally, neural nets can
       learn virtually anything (approximate any function), but their
       ability to do so depends highly on the nature of the
       inputs. &ldquo;Given a different set of inputs, these models might go
       straight to stage 2, or even regress from stage 2 to
       stage 1. It is at least not obvious that human infants enjoy
       the same degree of freedom.&rdquo;

</dd>
<dt>The power of learning algorithms</dt><dd>statistical approaches are
       often too good; humans do a poor job detecting statistical
       correlations; for example, we have a hard time learning how to
       efficiently match words to their reverse images, yet a
       statistical model could learn that perfectly; a good model of
       language acquisition should also explain what we cannot learn.

</dd>
<dt>Microfeature representations</dt><dd>humans do not seem to treat all
       features of an observation equally; we do not see a word as a
       sequence of equally-important letters; Pinker and Prince give
       the example, &ldquo;an animal that looks exactly like a skunk will
       nonetheless be treated as a raccoon if one is told that the
       stripe was painted onto an animal that had raccoon parents and
       raccoon babies.&rdquo;
</dd>
</dl>


<blockquote>

<p>The single most obvious phenomenon about the meanings of words is the
difficulty of focusing consciousness upon them. If I ask you what
does the verb &ldquo;sight&rdquo; mean in the sentence:
</p>
<p>
He sighted a herd of elephants on the plain
</p>
<p>
then you are immediately aware that you know the meaning of the word,
and that you understand the sentence with no difficulty. You should
also be able to offer a paraphrase of the word, such as:
</p>
<p>
to see something at a distance.
</p>
<p>
But the formulation of this paraphrase is not an immediate and
automatic process. You cannot turn to the appropriate definition in a
mental dictionary and read out the contents that you find there. It
may take a second or two to formulate a definition, and in some cases,
as we shall see, you may be unable to give a helpful definition at
all. In short, you have an immediate awareness of knowing the sense of
a word, but you have no direct introspect access to the representation
of its meaning.
</p>
</blockquote>


</div>
</div>

</div>

<div id="outline-container-7" class="outline-2">
<h2 id="sec-7">The edge of understanding</h2>
<div class="outline-text-2" id="text-7">


<p>
&ldquo;Outside a psychological laboratory, the recognition of words is
seldom an end in itself, because listeners want to understand what
they hear.&rdquo; (Johnson-Laird)
</p>



<pre class="example">a leaf
left
by the
cat
I guess

--- Aram Saroyan
</pre>



<pre class="example">My cup is yellow
Or not, though not's

Impossible
It's yellow

--- Aram Saroyan
</pre>



<pre class="example">I leaps

through
my eyes

--- Aram Saroyan
</pre>



<pre class="example">a window to walk away
in

--- Aram Saroyan
</pre>



<pre class="example">"Gamut"

Much ado about trees lichen
hugs alga and fungus live
off each other hoe does
dear owe dear earth terrace
money sunday coffee poorjoe snow

--- Louis Zukofsky
</pre>



<pre class="example">colorless green ideas sleep furiously

--- Noam Chomsky
</pre>



<pre class="example">Every farmer who owns a donkey beats it.

Every farmer who owns a tractor uses it to drive to church on Sundays.

Some farmers who own a donkey beat it.

--- infamous "donkey sentence" and variants
</pre>



<div style="text-align: center">
<p><img src="./images/kumi-yamashita-lovers.jpg"  alt="./images/kumi-yamashita-lovers.jpg" />
</p>
<p>
&ldquo;Lovers,&rdquo; <a href="http://www.kumiyamashita.com/portfolio/lovers/">Kumi Yamashita</a>, 1999; H190, W240, D15cm; cut aluminum plate,
single light source, shadow
</p>
</div>






<div style="font-size: 80%;">
<span xmlns:dct="http://purl.org/dc/terms/" href="http://purl.org/dc/dcmitype/Text" property="dct:title" rel="dct:type">CSE 630 material</span> by <a xmlns:cc="http://creativecommons.org/ns#" href="http://cse630.artifice.cc" property="cc:attributionName" rel="cc:attributionURL">Joshua Eckroth</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>. Source code for this website available at <a href="https://github.com/joshuaeckroth/cse630-website/tree/gh-pages">GitHub</a>.
</div>


</div>
</div>
</div>

</body>
</html>
